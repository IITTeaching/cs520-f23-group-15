{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444164e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f65ff525",
   "metadata": {},
   "source": [
    "<h1 align = \"center\"> Seasons of Cycling: Analyzing Divvy's Year-Long Data Trends</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197c1b0",
   "metadata": {},
   "source": [
    "# Data Collection and Preparation\n",
    "\n",
    "**Divvy Trip Data**<br>\n",
    "The datasets were downloaded from this [link](http://https://divvy-tripdata.s3.amazonaws.com/index.html).  A total of 12 csv files (1 file per month) were upload into Python as Pandas Dataframes.  The files were combined into 1 file using `.concat()` method.\n",
    "<!--\n",
    "**Chiacago Weather**\n",
    "Daily temperatures were obtained from this [dataset](https://www.kaggle.com/datasets/chuyuchen/midwest-cities-weather-data-2021).  The dataset was filtered to show only the city of Chicago and the columns: *datetime, temp, tempmax and tempmin*.  All numeric columns were also converted from Farenheit to Celcius.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db03205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5652827 entries, 0 to 5652826\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 560.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#read each csv file\n",
    "nov_22 = pd.read_csv(\"202211-divvy-tripdata.csv\")\n",
    "dec_22 = pd.read_csv(\"202212-divvy-tripdata.csv\")\n",
    "jan_23 = pd.read_csv(\"202301-divvy-tripdata.csv\")\n",
    "feb_23 = pd.read_csv(\"202302-divvy-tripdata.csv\")\n",
    "mar_23 = pd.read_csv(\"202303-divvy-tripdata.csv\")\n",
    "apr_23 = pd.read_csv(\"202304-divvy-tripdata.csv\")\n",
    "may_23 = pd.read_csv(\"202305-divvy-tripdata.csv\")\n",
    "jun_23 = pd.read_csv(\"202306-divvy-tripdata.csv\")\n",
    "jul_23 = pd.read_csv(\"202307-divvy-tripdata.csv\")\n",
    "aug_23 = pd.read_csv(\"202308-divvy-tripdata.csv\")\n",
    "sep_23 = pd.read_csv(\"202309-divvy-tripdata.csv\")\n",
    "oct_23 = pd.read_csv(\"202310-divvy-tripdata.csv\")\n",
    "print('import done')\n",
    "\n",
    "#use concat to combine 12 csv\n",
    "df=pd.concat([nov_22,dec_22,jan_23,feb_23,mar_23,apr_23,may_23,jun_23,jul_23,aug_23,sep_23,oct_23], ignore_index=True)\n",
    "#df = pd.concat([nov_22,feb_23,mar_23,apr_23], ignore_index=True)\n",
    "\n",
    "#drop unnecessary columns\n",
    "#df.drop(df.columns[[5, 7]], axis=1, inplace = True)\n",
    "\n",
    "#inspect dataframe\n",
    "#df.head()\n",
    "df.info()\n",
    "\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea472e63",
   "metadata": {},
   "source": [
    "The combined dataframe has 5 million records (5,652,827) and has 13 columns (attributes). Upon closer inspection it is observed that the started_at and ended_at columns are of incorrect datatype so we have converted them back to datetime datatype using pandas to_datetime() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8c47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e207cd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5652827 entries, 0 to 5652826\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   ride_id             object        \n",
      " 1   rideable_type       object        \n",
      " 2   started_at          datetime64[ns]\n",
      " 3   ended_at            datetime64[ns]\n",
      " 4   start_station_name  object        \n",
      " 5   start_station_id    object        \n",
      " 6   end_station_name    object        \n",
      " 7   end_station_id      object        \n",
      " 8   start_lat           float64       \n",
      " 9   start_lng           float64       \n",
      " 10  end_lat             float64       \n",
      " 11  end_lng             float64       \n",
      " 12  member_casual       object        \n",
      "dtypes: datetime64[ns](2), float64(4), object(7)\n",
      "memory usage: 560.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33aa3f",
   "metadata": {},
   "source": [
    "# Data Exploration and Cleaning\n",
    "\n",
    "Each record in the dataframe has ride_id along with time, station information like name, location(latitude and longitude), bike type and user_type for each ride recorded.\n",
    "<br>\n",
    "<br>\n",
    "**ride_id**, Let's talk about ride_id for a sec. Think of it as the VIP pass for each record in our data party â€“ it's unique for every guest and there are no plus-ones. The cool thing? Every ride_id is like a secret code, exactly 16 characters long. We've checked them all, and they're in perfect shape. So, guess what? We don't need to fuss over this column anymore. It's all good to go as is!\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6f99b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                column  unique_values  unique%  missing_values  missing%\n",
      "0              ride_id        5652827   100.00               0      0.00\n",
      "1        rideable_type              3     0.00               0      0.00\n",
      "2           started_at        4764981    84.29               0      0.00\n",
      "3             ended_at        4776456    84.50               0      0.00\n",
      "4   start_station_name           1579     0.03          866243     15.32\n",
      "5     start_station_id           1494     0.03          866375     15.33\n",
      "6     end_station_name           1589     0.03          918796     16.25\n",
      "7       end_station_id           1503     0.03          918937     16.26\n",
      "8            start_lat         784998    13.89               0      0.00\n",
      "9            start_lng         745056    13.18               0      0.00\n",
      "10             end_lat          13873     0.25            6759      0.12\n",
      "11             end_lng          13990     0.25            6759      0.12\n",
      "12       member_casual              2     0.00               0      0.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate % unique values per column\n",
    "duplicates = df.nunique().reset_index()\n",
    "duplicates.columns = ['column', 'unique_values']\n",
    "duplicates['unique%'] = round((duplicates['unique_values'] / len(df)) * 100, 2)\n",
    "\n",
    "# Calculate % missing values per column\n",
    "missing = df.isna().sum().reset_index()\n",
    "missing.columns = ['column', 'missing_values']\n",
    "missing['missing%'] = round((missing['missing_values'] / len(df)) * 100, 2)\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = pd.merge(duplicates, missing, on='column')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cd060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Total duplicates in ride_id column:  0\n"
     ]
    }
   ],
   "source": [
    "#checking if all the ride_id have exactly 16 charaters\n",
    "ride_id_len = (df['ride_id'].str.len()==16).all()\n",
    "print(ride_id_len)\n",
    "\n",
    "#count duplicates on unique column\n",
    "# this is done to find if there are any dupicate records present in the data as ride_id is the primary key, it is chosen\n",
    "print('Total duplicates in ride_id column: ',df['ride_id'].duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c1a86",
   "metadata": {},
   "source": [
    "**rideable_type**, this column is like the ID badge for each bike, telling us what model was used for the ride. We've got three types listed: classic_bike, docked_bike, and electric_bike. But here's an interesting update: we realized that 'docked_bike' is actually an old name for what we now call a 'classic_bike.' So, we decided to give our data a little makeover. We've updated 'docked_bike' to 'classic_bike' across the board, and this tweak has brought a new lease of life to 86098 records. It's all about keeping things consistent and clear!\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fcf4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['electric_bike' 'classic_bike' 'docked_bike']\n"
     ]
    }
   ],
   "source": [
    "#finding the unique values in rideable_type column\n",
    "print(df['rideable_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a55f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records having docked_bike type before: 86098\n",
      "The number of records having docked_bike type after the change: 0\n",
      "Total number of empty values or null values in rideable_type :  0\n"
     ]
    }
   ],
   "source": [
    "#counting the number of records that contained docked_bike\n",
    "count_before_change = (df['rideable_type'] == 'docked_bike').sum()\n",
    "print('The number of records having docked_bike type before:', count_before_change)\n",
    "#changing docked_bike to classic_bike\n",
    "df['rideable_type'] = df['rideable_type'].replace('docked_bike','classic_bike')\n",
    "#counting the number of records after change\n",
    "count_after_change = (df['rideable_type'] == 'docked_bike').sum()\n",
    "print('The number of records having docked_bike type after the change:', count_after_change)\n",
    "#checking for null values or empty values in rideable_type\n",
    "print('Total number of empty values or null values in rideable_type : ',df['rideable_type'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98876d",
   "metadata": {},
   "source": [
    "**started_at and ended_at**, these columns are our timekeepers in the dataset. They don't just tell us when each bike trip kicked off and wrapped up; they're the key to unlocking much more. With these timestamps, we can calculate the duration of each ride â€“ a vital piece of the puzzle. But there's more: by breaking down these dates and times, we can see patterns based on the day of the week and specific dates. This slicing and dicing of time not only makes our data richer for analysis but also adds a dash of life to our data visualizations. The granularity we get from these details is super valuable, helping us spot trends and derive insights that would otherwise be hidden in broader data.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c12833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>ride_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BCC66FC6FAB27CC7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-11-10 06:21:55</td>\n",
       "      <td>2022-11-10 06:31:27</td>\n",
       "      <td>Canal St &amp; Adams St</td>\n",
       "      <td>13011</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>13016</td>\n",
       "      <td>41.879401</td>\n",
       "      <td>-87.639848</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>9.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>772AB67E902C180F</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-11-04 07:31:55</td>\n",
       "      <td>2022-11-04 07:46:25</td>\n",
       "      <td>Canal St &amp; Adams St</td>\n",
       "      <td>13011</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>13016</td>\n",
       "      <td>41.879255</td>\n",
       "      <td>-87.639904</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>Friday</td>\n",
       "      <td>14.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>585EAD07FDEC0152</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-11-21 17:20:29</td>\n",
       "      <td>2022-11-21 17:34:36</td>\n",
       "      <td>Indiana Ave &amp; Roosevelt Rd</td>\n",
       "      <td>SL-005</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>13016</td>\n",
       "      <td>41.867888</td>\n",
       "      <td>-87.623041</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>Monday</td>\n",
       "      <td>14.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91C4E7ED3C262FF9</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-11-25 17:29:34</td>\n",
       "      <td>2022-11-25 17:45:15</td>\n",
       "      <td>Indiana Ave &amp; Roosevelt Rd</td>\n",
       "      <td>SL-005</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>13016</td>\n",
       "      <td>41.867888</td>\n",
       "      <td>-87.623041</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>Friday</td>\n",
       "      <td>15.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>709206A3104CABC8</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-11-29 17:24:25</td>\n",
       "      <td>2022-11-29 17:42:51</td>\n",
       "      <td>Indiana Ave &amp; Roosevelt Rd</td>\n",
       "      <td>SL-005</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>13016</td>\n",
       "      <td>41.867888</td>\n",
       "      <td>-87.623041</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>18.433333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type          started_at            ended_at  \\\n",
       "0  BCC66FC6FAB27CC7  electric_bike 2022-11-10 06:21:55 2022-11-10 06:31:27   \n",
       "1  772AB67E902C180F   classic_bike 2022-11-04 07:31:55 2022-11-04 07:46:25   \n",
       "2  585EAD07FDEC0152   classic_bike 2022-11-21 17:20:29 2022-11-21 17:34:36   \n",
       "3  91C4E7ED3C262FF9   classic_bike 2022-11-25 17:29:34 2022-11-25 17:45:15   \n",
       "4  709206A3104CABC8   classic_bike 2022-11-29 17:24:25 2022-11-29 17:42:51   \n",
       "\n",
       "           start_station_name start_station_id        end_station_name  \\\n",
       "0         Canal St & Adams St            13011  St. Clair St & Erie St   \n",
       "1         Canal St & Adams St            13011  St. Clair St & Erie St   \n",
       "2  Indiana Ave & Roosevelt Rd           SL-005  St. Clair St & Erie St   \n",
       "3  Indiana Ave & Roosevelt Rd           SL-005  St. Clair St & Erie St   \n",
       "4  Indiana Ave & Roosevelt Rd           SL-005  St. Clair St & Erie St   \n",
       "\n",
       "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \\\n",
       "0          13016  41.879401 -87.639848  41.894345 -87.622798        member   \n",
       "1          13016  41.879255 -87.639904  41.894345 -87.622798        member   \n",
       "2          13016  41.867888 -87.623041  41.894345 -87.622798        member   \n",
       "3          13016  41.867888 -87.623041  41.894345 -87.622798        member   \n",
       "4          13016  41.867888 -87.623041  41.894345 -87.622798        member   \n",
       "\n",
       "         date       day  ride_duration  \n",
       "0  2022-11-10  Thursday       9.533333  \n",
       "1  2022-11-04    Friday      14.500000  \n",
       "2  2022-11-21    Monday      14.116667  \n",
       "3  2022-11-25    Friday      15.683333  \n",
       "4  2022-11-29   Tuesday      18.433333  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have already converted the started_at and ended_at columns to datetime format and hence we can proceed to find the duration of the ride \n",
    "df['date'] = df['started_at'].dt.date\n",
    "df['day'] = df['started_at'].dt.day_name()\n",
    "df['ride_duration'] = ((df['ended_at'] - df['started_at']).dt.total_seconds() / 60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1ca4d",
   "metadata": {},
   "source": [
    "### Dealing with outliers\n",
    "\n",
    "Here's a curious thing we spotted: among the sea of rides, some are as brief as under a minute, while others stretch beyond 24 hours â€“ talk about extremes! We've decided to label these ultra-short and ultra-long rides as outliers. It's like finding a needle in a haystack, but we did it â€“ and to keep our data neat and tidy, we're going to remove these outliers. A total of 156,036 rows, to be exact, are saying goodbye to our main dataset. But don't worry, they're not going into the data void; we're giving them a new home in a separate dataframe, df_duration_noise. This move helps us focus on the more typical rides and maintain the integrity of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "585a4cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This change has resulted in deleting 156036 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(156036, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count rows before filtering\n",
    "count_before = len(df)\n",
    "\n",
    "# First, create a DataFrame of outliers\n",
    "df_duration_noise = df[(df['ride_duration'] < 1) | (df['ride_duration'] > 24*60)]\n",
    "\n",
    "# Then, filter df to remove outliers\n",
    "df = df[(df['ride_duration'] >= 1) & (df['ride_duration'] <= 24*60)]\n",
    "\n",
    "# Count rows after filtering\n",
    "count_after = len(df)\n",
    "\n",
    "# Print the number of rows deleted\n",
    "print('This change has resulted in deleting', count_before - count_after, 'rows')\n",
    "\n",
    "# Check the shape of df_duration_noise\n",
    "df_duration_noise.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435a821",
   "metadata": {},
   "source": [
    "**start_station_name and end_station_name**<br> We've noticed a bit of a puzzle: quite a few trips are missing either their starting or ending station names. Now, here's where things get interesting. For classic bikes, it's a must to have both a start and an end at a docking station. But electric bikes? They're the free spirits of our dataset â€“ they can end their journeys pretty much anywhere, no dock required. So, to keep our data tidy and meaningful, we've made a decision: any classic bike records missing station names are going to be moved to a new home, a separate dataframe we're calling df_station_noise. This way, we keep our main dataset clean and focused on the complete journeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9793f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify classic bike records missing station names\n",
    "classic_missing_stations = (df['rideable_type'] == 'classic') & (df['start_station_name'].isna() | df['end_station_name'].isna())\n",
    "\n",
    "# Create df_station_noise DataFrame\n",
    "df_station_noise = df[classic_missing_stations].copy()\n",
    "\n",
    "# Update the main DataFrame by removing these records\n",
    "df = df[~classic_missing_stations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2449ad7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>ride_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>7F8CA9B17D7E2B5F</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-11-03 11:52:03</td>\n",
       "      <td>2022-11-03 11:52:47</td>\n",
       "      <td>Desplaines St &amp; Kinzie St</td>\n",
       "      <td>TA1306000003</td>\n",
       "      <td>Desplaines St &amp; Kinzie St</td>\n",
       "      <td>TA1306000003</td>\n",
       "      <td>41.888716</td>\n",
       "      <td>-87.644448</td>\n",
       "      <td>41.888716</td>\n",
       "      <td>-87.644448</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>9320FCC9994902BC</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-11-08 05:17:18</td>\n",
       "      <td>2022-11-08 05:17:21</td>\n",
       "      <td>Hoyne Ave &amp; Balmoral Ave</td>\n",
       "      <td>655</td>\n",
       "      <td>Hoyne Ave &amp; Balmoral Ave</td>\n",
       "      <td>655</td>\n",
       "      <td>41.979913</td>\n",
       "      <td>-87.682015</td>\n",
       "      <td>41.979851</td>\n",
       "      <td>-87.681932</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>E7372C2C8A9BFCA7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-11-08 05:16:41</td>\n",
       "      <td>2022-11-08 05:16:45</td>\n",
       "      <td>Hoyne Ave &amp; Balmoral Ave</td>\n",
       "      <td>655</td>\n",
       "      <td>Hoyne Ave &amp; Balmoral Ave</td>\n",
       "      <td>655</td>\n",
       "      <td>41.979833</td>\n",
       "      <td>-87.682010</td>\n",
       "      <td>41.979851</td>\n",
       "      <td>-87.681932</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2B096F11BFFAEEF4</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2022-11-25 10:47:39</td>\n",
       "      <td>2022-11-25 10:48:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ashland Ave &amp; Lake St</td>\n",
       "      <td>13073</td>\n",
       "      <td>41.890000</td>\n",
       "      <td>-87.670000</td>\n",
       "      <td>41.885920</td>\n",
       "      <td>-87.667170</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>61A73ABE32A0FFE6</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2022-11-03 15:49:17</td>\n",
       "      <td>2022-11-03 15:49:19</td>\n",
       "      <td>Walsh Park</td>\n",
       "      <td>18067</td>\n",
       "      <td>Walsh Park</td>\n",
       "      <td>18067</td>\n",
       "      <td>41.914610</td>\n",
       "      <td>-87.667968</td>\n",
       "      <td>41.914610</td>\n",
       "      <td>-87.667968</td>\n",
       "      <td>member</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ride_id  rideable_type          started_at            ended_at  \\\n",
       "149  7F8CA9B17D7E2B5F   classic_bike 2022-11-03 11:52:03 2022-11-03 11:52:47   \n",
       "151  9320FCC9994902BC  electric_bike 2022-11-08 05:17:18 2022-11-08 05:17:21   \n",
       "152  E7372C2C8A9BFCA7  electric_bike 2022-11-08 05:16:41 2022-11-08 05:16:45   \n",
       "189  2B096F11BFFAEEF4  electric_bike 2022-11-25 10:47:39 2022-11-25 10:48:05   \n",
       "412  61A73ABE32A0FFE6   classic_bike 2022-11-03 15:49:17 2022-11-03 15:49:19   \n",
       "\n",
       "            start_station_name start_station_id           end_station_name  \\\n",
       "149  Desplaines St & Kinzie St     TA1306000003  Desplaines St & Kinzie St   \n",
       "151   Hoyne Ave & Balmoral Ave              655   Hoyne Ave & Balmoral Ave   \n",
       "152   Hoyne Ave & Balmoral Ave              655   Hoyne Ave & Balmoral Ave   \n",
       "189                        NaN              NaN      Ashland Ave & Lake St   \n",
       "412                 Walsh Park            18067                 Walsh Park   \n",
       "\n",
       "    end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \\\n",
       "149   TA1306000003  41.888716 -87.644448  41.888716 -87.644448        member   \n",
       "151            655  41.979913 -87.682015  41.979851 -87.681932        member   \n",
       "152            655  41.979833 -87.682010  41.979851 -87.681932        member   \n",
       "189          13073  41.890000 -87.670000  41.885920 -87.667170        member   \n",
       "412          18067  41.914610 -87.667968  41.914610 -87.667968        member   \n",
       "\n",
       "           date       day  ride_duration  \n",
       "149  2022-11-03  Thursday       0.733333  \n",
       "151  2022-11-08   Tuesday       0.050000  \n",
       "152  2022-11-08   Tuesday       0.066667  \n",
       "189  2022-11-25    Friday       0.433333  \n",
       "412  2022-11-03  Thursday       0.033333  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#storing the outliers or noise in a dataframe called df_noise, this is done to preserve the data integrity and future analysis\n",
    "df_noise = pd.concat([df_duration_noise, df_station_noise])\n",
    "df_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b18f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156036, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14a8b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['member' 'casual']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking member_casual column for possible values\n",
    "print((df['member_casual']).unique())\n",
    "df['member_casual'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340a7fe",
   "metadata": {},
   "source": [
    "In our journey through the data, we've come across a neat little detail about the member_casual column. It turns out, it's pretty straightforward â€“ just two types of riders here, member and casual. And guess what? There's not a single null or empty spot in sight for this column.\n",
    "\n",
    "But here's where it gets a bit more complex: the start_station_name, start_station_id, end_station_name, and end_station_id columns are a different story. They've got a fair share of nulls and empties. However, we've decided not to show these records the exit door. Why? Because these gaps actually tell us something important â€“ they reflect the unique flexibility of electric bikes, which don't always need a specific docking station. So, instead of dropping this valuable info, we've taken a creative turn: we're labeling these unknowns with an unknown value. This way, we acknowledge the gaps without losing the bigger picture of our bike-riding saga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9976c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding the number of missing values and their percentage after removing the outliers and noise\n",
      "                column  missing_values  missing%\n",
      "0              ride_id               0      0.00\n",
      "1        rideable_type               0      0.00\n",
      "2           started_at               0      0.00\n",
      "3             ended_at               0      0.00\n",
      "4   start_station_name          821489     14.94\n",
      "5     start_station_id          821614     14.95\n",
      "6     end_station_name          853288     15.52\n",
      "7       end_station_id          853426     15.53\n",
      "8            start_lat               0      0.00\n",
      "9            start_lng               0      0.00\n",
      "10             end_lat             801      0.01\n",
      "11             end_lng             801      0.01\n",
      "12       member_casual               0      0.00\n",
      "13                date               0      0.00\n",
      "14                 day               0      0.00\n",
      "15       ride_duration               0      0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"finding the number of missing values and their percentage after removing the outliers and noise\")\n",
    "missing = df.isna().sum().reset_index()\n",
    "missing.columns = ['column', 'missing_values']\n",
    "missing['missing%'] = round((missing['missing_values'] / len(df)) * 100, 2)\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e9fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to replace missing values with 'unknown'\n",
    "columns_to_replace = ['start_station_name', 'start_station_id', 'end_station_name', 'end_station_id']\n",
    "\n",
    "# Replace NaN values in each specified column with 'unknown'\n",
    "for column in columns_to_replace:\n",
    "    df[column] = df[column].fillna('unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018f80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned-divvy-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb8c99",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Now moving on to the data visualization using the processed data to derive insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8426590",
   "metadata": {},
   "source": [
    "ride_counts = df['member_casual'].value_counts()\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = ride_counts.index\n",
    "sizes = ride_counts.values\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # Ensures the pie chart is circular\n",
    "plt.title('Distribution of Rides: Members vs. Casual Riders')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f675f",
   "metadata": {},
   "source": [
    "average_ride_duration = df.groupby('member_casual')['ride_duration'].mean()\n",
    "print(average_ride_duration)\n",
    "average_ride_duration = {'Member': 21.198452, 'Casual': 12.371359}  # in minutes\n",
    "'''\n",
    "\n",
    "# Data preparation\n",
    "categories = list(average_ride_duration.keys())\n",
    "values = list(average_ride_duration.values())\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(categories, values, color=['blue', 'green'])\n",
    "plt.xlabel('Rider Type')\n",
    "plt.ylabel('Average Ride Duration (minutes)')\n",
    "plt.title('Average Ride Duration: Member vs Casual Riders')\n",
    "plt.xticks(categories)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038c95b",
   "metadata": {},
   "source": [
    "# Define a function to categorize days into 'Weekday' and 'Weekend'\n",
    "def categorize_day(day):\n",
    "    if day.weekday() < 5:  # 0 to 4 corresponds to Monday to Friday\n",
    "        return 'Weekday'\n",
    "    else:  # 5 and 6 corresponds to Saturday and Sunday\n",
    "        return 'Weekend'\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['day_type'] = df['date'].apply(categorize_day)\n",
    "\n",
    "# Group by rider type and day type, then calculate the mean duration\n",
    "average_duration = df.groupby(['member_casual', 'day_type'])['ride_duration'].mean().reset_index()\n",
    "\n",
    "# Pivot the data for easier plotting\n",
    "pivot_data = average_duration.pivot(index='day_type', columns='member_casual', values='ride_duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a63a58",
   "metadata": {},
   "source": [
    "# Plotting the data\n",
    "pivot_data.plot(kind='bar', figsize=(10, 6))\n",
    "\n",
    "plt.xlabel('Day Type')\n",
    "plt.ylabel('Average Ride Duration')\n",
    "plt.title('Average Ride Duration by Rider Type and Day Type')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels to show them horizontally\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119ac2e",
   "metadata": {},
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# Create a new column for the day of the week\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "# Group by day of the week and rider type, then count the trips\n",
    "trips_by_day_rider = df.groupby(['day_of_week', 'member_casual']).size().reset_index(name='trip_count')\n",
    "\n",
    "# Pivot the data for easier plotting\n",
    "pivot_data = trips_by_day_rider.pivot(index='day_of_week', columns='member_casual', values='trip_count')\n",
    "\n",
    "# Ensure the days are ordered\n",
    "days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "pivot_data = pivot_data.reindex(days_order)\n",
    "\n",
    "# Plotting the data\n",
    "pivot_data.plot(kind='line', marker='o', figsize=(10, 6))\n",
    "\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.title('Number of Trips by Day of the Week and Rider Type')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(len(days_order)), days_order)  # Set x-ticks to days of the week\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56a43d",
   "metadata": {},
   "source": [
    "From the graph, we can infer the following:\n",
    "\n",
    "**Casual Riders:**\n",
    "\n",
    "The number of trips by casual riders is higher than that of members on every day of the week.\n",
    "Casual ridership appears to peak midweek, with the highest number on Wednesday, and then gradually declines towards the weekend.\n",
    "\n",
    "**Member Riders:**\n",
    "\n",
    "The pattern for member riders is quite different. The number of trips starts low on Monday, increases significantly on Tuesday, remains relatively steady through Friday, and then spikes on Saturday.\n",
    "The number of trips for members drops on Sunday, indicating perhaps a lesser preference for using the service on that day compared to Saturday.\n",
    "\n",
    "**Overall Trends:**\n",
    "\n",
    "Casual riders seem to use the service more consistently across the week with a peak in the middle of the week.\n",
    "Members show a preference for using the service towards the end of the workweek and on Saturdays.\n",
    "\n",
    "<strong>*These observations can suggest several underlying behaviors and preferences:*</strong>\n",
    "<ol>\n",
    "<li>Casual riders might include tourists or occasional users who are more active during the week, possibly indicating leisure or errand-related activities that are not tied to the workweek schedule.</li>\n",
    "<li>Member riders may use the service for commuting purposes, reflected in the higher number of trips on weekdays, especially towards the latter half of the week and on Saturdays for weekend activities.\n",
    "    </li>\n",
    "<li>The drop in member trips on Sunday might indicate a day of rest or non-reliance on bike-sharing services, possibly due to reduced work or social activities.</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d8c23",
   "metadata": {},
   "source": [
    "'''\n",
    "!pip install pivottablejs\n",
    "from pivottablejs import pivot_ui\n",
    "pivot_ui(df.head(20000))\n",
    "pivot_ui(df_noise)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532b61d",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Creating the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_data, annot=True, fmt=\"d\", cmap='YlGnBu', linewidths=.5)\n",
    "\n",
    "plt.title('Heatmap of Number of Trips by Day of the Week and Rider Type')\n",
    "plt.ylabel('Day of the Week')\n",
    "plt.xlabel('Rider Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba702a98",
   "metadata": {},
   "source": [
    "The heatmap illustrates the distribution of Divvy bike-sharing trips across different days of the week for casual and member riders. Members show a consistent increase in trips as the week progresses, peaking on Tuesday, while casual riders' trips peak on Saturdays. The heatmap's color gradient indicates that members generally take more trips than casual riders on weekdays, with the highest volume on Tuesday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6bc94",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "z_scores = stats.zscore(df['ride_duration'])\n",
    "outliers_z = df[(z_scores < -3) | (z_scores > 3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa705af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd31a3a7",
   "metadata": {},
   "source": [
    "Q1 = df['ride_duration'].quantile(0.25)\n",
    "Q3 = df['ride_duration'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers_iqr = df[(df['ride_duration'] < (Q1 - 1.5 * IQR)) | (df['ride_duration'] > (Q3 + 1.5 * IQR))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0ff58",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(df['ride_duration'])\n",
    "plt.title('Boxplot of Ride Durations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28384ce5",
   "metadata": {},
   "source": [
    "z-score outlier detection and IQR outlier detection are statistical methods used to identify abnormal points in the dataset, specifically within the ride_duration variable:\n",
    "\n",
    "**Z-Score Outlier Detection:**\n",
    "\n",
    "The z-score method standardizes the entire dataset by converting data points into z-scores, which represent the number of standard deviations a point is from the mean.\n",
    "This project uses a threshold of 3 standard deviations to identify outliers, meaning any ride duration more than 3 standard deviations from the mean ride duration is considered an outlier.\n",
    "This method is sensitive to the mean and standard deviation, and therefore can be affected by extreme values or a non-normal distribution of data.\n",
    "\n",
    "**IQR Outlier Detection:**\n",
    "\n",
    "The IQR method involves calculating the interquartile range, which is the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the data.\n",
    "Outliers are defined as observations that fall below Q1 - 1.5IQR or above Q3 + 1.5IQR. This does not assume a normal distribution of the data and is less influenced by extreme values.\n",
    "In this project, any ride_duration outside of these bounds is classified as an outlier and is a candidate for exclusion from further analysis to prevent skewing the results.\n",
    "\n",
    "**Boxplot Visualization:**\n",
    "\n",
    "The boxplot created in the project provides a visual representation of the distribution of ride_duration.\n",
    "It displays the median, quartiles, and potential outliers, which are individual points that appear outside the whiskers of the boxplot (typically set at 1.5*IQR from the quartiles).\n",
    "This visualization aids in confirming the presence of outliers and understanding the spread and symmetry of the data.\n",
    "\n",
    "Both z-scores and IQR are used here to rigorously identify ride durations that are unusually long or short, which might otherwise bias the analysis. The boxplot serves as a visual confirmation of these findings, offering a clear picture of the data distribution and highlighting any potential outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509663d6",
   "metadata": {},
   "source": [
    "#time series analysis \n",
    "#This plots the average ride duration for each month to observe any trends over time.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#df['month'] = df['started_at'].dt.to_period('M')\n",
    "# Ensure 'ride_duration' is numeric\n",
    "df['ride_duration'] = pd.to_numeric(df['ride_duration'], errors='coerce')\n",
    "   \n",
    "# Convert 'started_at' to Period (monthly), then to string for plotting\n",
    "df['month'] = df['started_at'].dt.to_period('M').astype(str)\n",
    "\n",
    "monthly_duration = df.groupby('month')['ride_duration'].mean().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=monthly_duration, x='month', y='ride_duration')\n",
    "plt.title('Average Ride Duration Per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Ride Duration (minutes)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8834e7a",
   "metadata": {},
   "source": [
    "The line graph depicts the average ride duration per month for Divvy bike-sharing from November 2022 to October 2023. There is a noticeable dip in ride durations in December 2022, after which there's a steady increase, peaking in July 2023. Following this peak, there's a sharp decline in August and September, with a slight recovery in October. This trend may suggest seasonal patterns in ride usage, with longer rides in the warmer months and shorter rides in the colder months.\n",
    "\n",
    "The observed pattern likely reflects user behavior changes in response to weather conditions, with shorter rides during the cold winter months and longer rides during the warm summer months, indicating a preference for using bike-sharing services for longer periods when the weather is more favorable. The decline in late summer could be due to a return to school or work routines, suggesting a shift in the reasons for bike usage. The data could be vital for Divvy in planning resource allocation, and maintenance schedules to match these seasonal trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d1dd2",
   "metadata": {},
   "source": [
    "#rider behaviour analysis\n",
    "#This will compare the ride durations between members and casual riders across days of the week.\n",
    "df['day_of_week'] = df['started_at'].dt.day_name()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df, x='day_of_week', y='ride_duration', hue='member_casual')\n",
    "plt.title('Ride Duration Comparison by Rider Type Across Days of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Ride Duration (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80477f",
   "metadata": {},
   "source": [
    "#Geospatial analysis\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Create a base map\n",
    "map = folium.Map(location=[df['start_lat'].mean(), df['start_lng'].mean()], zoom_start=12)\n",
    "\n",
    "# Add a heatmap to the base map\n",
    "HeatMap(data=df[['start_lat', 'start_lng']].dropna(), radius=10).add_to(map)\n",
    "\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
